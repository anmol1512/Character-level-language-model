# -*- coding: utf-8 -*-
"""character-level Transformer LM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZ5sxia_jzwijlV5TXL6HGzjX2-hyZNo

## Single Head self-attention block
"""

import torch as tt
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt

n_embds=32
head_size=16




class Head(nn.Module):
  def __init__(self,head_size):
    super().__init__()
    '''self-attention layers[query,key and value]'''
    self.query=nn.Linear(n_embds,head_size,bias=False)
    self.key=nn.Linear(n_embds,head_size,bias=False)
    self.value=nn.Linear(n_embds,head_size,bias=False)

    '''lower triangular matrix is registered as buffer for masking purpose'''
    self.register_buffer('lower_tri',tt.tril(tt.ones((block_size,block_size),dtype=tt.long))) # (T,T) tensor is created and registered as buffer.

  def forward(self,x): # x is the encoding of its identity+its position.
    q=self.query(x)
    k=self.key(x)
    wei=(q @ k.transpose(-2,-1)) * q.shape[-1]**0.5#(B,T,T)
    '''wei is known as attention score as every token attends every other token,
     and then we normalize it and called them as scaled attention'''

    # -----DECODER-----
    wei=wei.masked_fill(lower_tri==0,float('-inf')) #(B,T,T)
    wei=F.softmax(wei,dim=1) # Aggregating all the previous token information

    v=self.value(x)
    out=wei @ v # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)
    return out # input(B,T,n_embd) -> out(B,T,head_size)







class gptmodel(nn.Module):
  def __init__(self):
    super().__init__()
    '''Encoding the input token sequence'''
    self.token_embedding=nn.Embedding(vocab_size,n_embds)
    self.position_embedding=nn.Embedding(block_size,n_embds)

    '''Communication mechanism so that tokens can attend to its previous tokens.'''
    self.sa_head=Head(head_size)

    '''Decoding the information loaded encoded token sequence via transformation.'''
    self.lm_head=nn.Linear(n_embds,vocab_size)

  def forward(self,xb):
    B,T=xb.shape
    token_embd=self.token_embedding(x) #(B,T) -> (B,T,nmbds)
    position_embd=self.position_embedding(tt.arange(T,device=device)) #(T) -> (T,n_embds)
    x=token_embd+position_embd

    x_attend=self.sa_head(x)

    logits=self.lm_head(x_attend)
    return logits

  def generate(self,max_token):
    for tk in range(max_token):
      logits,_=self(idx) #(B,T) -> (B,T,vocab_size)
      logits=logits[:,-1,:] #(B,T,vocab_size) -> (B,vocab_size)
      prob=F.softmax(logits,dim=-1) #(B,vocab_size) -> (B,vocab_size)
      next_idx=tt.multinomial(prob,num_samples=1) #(B,vocab_size) -> (B,1)
      idx=tt.cat((idx,next_idx),dim=-1) #(B,T) ->(B,T+1)