# -*- coding: utf-8 -*-
"""character-level Transformer LM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZ5sxia_jzwijlV5TXL6HGzjX2-hyZNo

## Decoder only Tranformer
"""

import torch as tt
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import time
import math
from .utils import CfgNode as CN

batch_size= 128
block_size=256
learning_rate=1e-4
epochs=5000
eval_interval=500
eval_iter=300
n_embds=512
n_heads=8
max_token=2000
n_decoder_layer=8

attn_drop_ratio=0.2
proj_drop_ratio=0.2
embd_drop_ratio=0.2

tt.manual_seed(1512)

# Input dataset containing all the work of shakespeare
# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

# Reading the file f-->input.txt
with open('input.txt','r') as f:
  input=f.read()

print("Total number of character in the file:",len(input))
# first 100 characters in the input file
print("preview of the input file:",end='\n\n')
print(input[:100])

# Defining vocabulary for creating a tokenizer
device = tt.device('cuda:0') if tt.cuda.is_available() else 'cpu'
vocab=sorted(list(set(input)))
vocab_size=len(vocab)
print('vocab: '+''.join(vocab))
print(f'voacb size: {vocab_size}')
print(f'device: {device.get_device_name() if device==tt.device("cuda:0") else device}')

# Our own character level tokenizer
ctoi={ele:ind for ind,ele in enumerate(vocab)}
itoc={ind:ele for ind,ele in enumerate(vocab)}
encode=lambda text:[ctoi[char] for char in text]
decode=lambda token_list:''.join([itoc[token] for token in token_list])


# Our charecter based tokenizer implementation/demo
e=encode("anmol is great")
d=decode(e)
print(e)
print(d)

# Encoded the input file into tokens of integer sequence
data=tt.tensor(encode(input),dtype=tt.long)
print(data.shape)
print(data[:10])
print(input[:10])

# Spliting the encode tensor data into training data and validation data
n=int(0.9*len(data))
train_data=data[:n]
val_data=data[n:]

# Relation between context and target
block_size=8
x=train_data[:block_size]
y=train_data[1:block_size+1]
for i in range(block_size):
  context=x[:i+1]
  target=y[i]
  print(f'when input is {context} the target is: {target}')

# Data Loader Implementation
tt.manual_seed(1512)

def get_batch(data_type):
  data=train_data if data_type=='train' else val_data
  x_start=tt.randint(high=len(data)-block_size,size=(batch_size,))
  x= tt.stack([data[i:i+block_size] for i in x_start])
  y= tt.stack([data[i+1:i+block_size+1] for i in x_start])
  x=x.to(device)
  y=y.to(device)
  return x,y



xb,yb=get_batch('train')
print(xb.shape)
print(yb.shape)

# Custom implementation of Layer normalization layer by scratch
class LayerNorm:
  def __init__(self,dim,eps=1e-3):
    self.eps=eps
    self.gamma=tt.ones(dim,device=device)
    self.beta=tt.zeros(dim,device=device)
  def __call__(self,x):
    xmean=x.mean(-1,keepdim=True)
    xvar=x.var(-1,keepdim=True)
    xhat=(x-xmean)/(tt.sqrt(xvar+self.eps))
    out=xhat*self.gamma+self.beta
    return out
  def parameters(self):
    return {'gamma': self.gamma,'beta': self.beta}



"""**Multi-Head self attention**

In the context of the multi-head self-attention mechanism in the Transformer architecture, the idea is to project the input into multiple subspaces (heads), each with its own learnable parameters. Each head computes a separate attention distribution, capturing different aspects of the relationships within the input sequence. The outputs from all heads are then concatenated and linearly transformed to produce the final output.

**Advantages**
1.  It acts as a form of regularization, making the model less sensitive to noise in the training data.
2. The attention weights produced by different heads can provide insights into which parts of the input sequence are relevant for specific heads.
3. Parallelization or parallel processing is computationally efficient in terms of usage of hardware resources and training time.
4. Reduce overfitting and makes model more robust and capable of generalizing well to different types of data
"""


""" **Feed forward neural network**
1. A feedforward neural network is a type of artificial neural network in which nodes' connections do not form a loop.
2. Often referred to as a multi-layered network of neurons.
3. feedforward neural networks are so named because all information flows in a forward manner only.
"""
#causal(unidirectional) self attention
class SelfAttention(nn.Module):
  '''Communication mechanism so that tokens can attend to its previous tokens.'''

  def __init__(self,n_heads,total_hs):
    super().__init__()
    self.n_heads=n_heads
    self.total_hs=total_hs
    assert self.total_hs%self.n_heads==0
    self.attn_vectors=nn.Linear(n_embds,3*self.total_hs,bias=False)
    self.attn_drop=nn.Dropout(attn_drop_ratio)
    self.proj_drop=nn.Dropout(proj_drop_ratio)
    self.proj=nn.Linear(self.total_hs,n_embds)

    self.register_buffer('lower_tri',tt.tril(tt.ones(1,1,block_size,block_size,dtype=tt.long)))
    # lower triangular matrix is created and registered as buffer.

  def forward(self,x_embd):
    B,T,C=x_embd.shape #(B,T,n_embds)
    q,k,v=self.attn_vectors(x_embd).split(self.total_hs,dim=-1)

    q=q.view(B,T,self.n_heads,q.shape[-1]//self.n_heads).transpose(1,2) #(B,n_heads,T,head_size)
    k=k.view(B,T,self.n_heads,k.shape[-1]//self.n_heads).transpose(1,2) #(B,n_heads,T,head_size)
    v=v.view(B,T,self.n_heads,v.shape[-1]//self.n_heads).transpose(1,2) #(B,n_heads,T,head_size)
    wei=q@k.transpose(-2,-1) # Dot product (compatibility matrix) (B,n_heads,T,T)
    wei/=math.sqrt(q.shape[-1]) # Scaled dot product (B,n_heads,T,T)
    masked_wei=wei.masked_fill(self.lower_tri[:,:,:T,:T]==0,float('-inf')) # (B,n_heads,T,T)
    soft_wei=F.softmax(masked_wei,dim=-1) # (B,n_heads,T,T) attending only the current and its previous tokens.
    soft_wei=self.attn_drop(soft_wei)
    
    out=soft_wei@v # (B,n_heads,T,head_size)
    out=out.transpose(1,2).contiguous().view(B,T,C)

    project_out=self.proj(out)

    return self.proj_drop(project_out)

#(multi-head SA +FFNN)
class TransformerBlock(nn.Module):
  # communication followed by computation
  def __init__(self,n_heads,total_head_size):
    super().__init__()
    self.sa_heads=SelfAttention(n_heads,total_head_size)

    '''Feed forward neural network block to think on the comunicated information
       operating on token level individually'''
    self.ann=nn.ModuleDict(dict(
      hidden=nn.Linear(n_embds,4*n_embds),
      proj=nn.Linear(4*n_embds,n_embds),
      hidden_active=nn.ReLU(),
      neural_drop=nn.Dropout(proj_drop_ratio)
    ))

    self.ln1=nn.LayerNorm(n_embds)
    self.ln2=nn.LayerNorm(n_embds)

    self.neural_net=lambda x:self.ann.neural_drop(self.ann.proj(self.ann.hidden_active(self.ann.hidden(x))))

  def forward(self,x):
    x_attend=x+self.sa_heads(self.ln1(x))
    x_think=x_attend+self.neural_net(self.ln2(x_attend))
    return x_think


class nanogptmodel(nn.Module):
  '''GPT Language Model'''

  @staticmethod
  def get_default_config():
    '''All Hyperparameters related to transformer model'''
    config=CN()
    # either model_type or (n_decoder_layer,n_heads,n_embds) should be given
    config.model_type='gpt'
    config.n_decoder_layer=None
    config.n_heads=None
    config.n_embds=None

    #below config should be provided externally
    config.vocab_size=None
    config.block_size=None
    
    #Dropout Hyperparameters
    config.attn_drop_ratio=0.2
    config.proj_drop_ratio=0.2
    config.embd_drop_ratio=0.2

    return config

  def __init__(self,config):
    super().__init__()

    assert config.vocab_size is not None
    assert config.block_size is not None

    block_params_checker=all([config.n_decoder_layer is not None,config.n_heads is not None,config.n_embds is not None])
    model_type_checker=config.model_type is not None
    assert model_type_checker^block_params_checker

    if model_type_checker:
      '''translate from model type to its corresponding configuration'''
      config._update_dict({
        #GPT1
        'openai-gpt' :  dict(n_decoder_layer=12,n_heads=12,n_embds=768),
        #GPT2
        'gpt2' :        dict(n_decoder_layer=12,n_heads=12,n_embds=768),  #124M parameters.
        'gpt2-medium' : dict(n_decoder_layer=24,n_heads=16,n_embds=1024), #355M parameter.
        'gpt2-large' :  dict(n_decoder_layer=36,n_heads=20,n_embds=1280), #774M parameter.
        'gpt2-xl' :     dict(n_decoder_layer=12,n_heads=12,n_embds=768),  # 1.5B parameter.
        #Tiny custom version of GPT
        'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),
        'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),
        'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),
      }[config.model_type])


    '''Encoding the input token sequence t_em with position encoding p_em'''
    '''then applying 'n_decoder_layer' transformer block followed by a layer normalization layer'''
    self.transformer=nn.ModuleDict(dict(
      t_em=nn.Embedding(config.vocab_size,config.n_embds), 
      p_em=nn.Embedding(config.block_size,config.n_embds),
      embd_drop=nn.Dropout(config.embd_drop_ratio),
      block=nn.ModuleList([TransformerBlock(config.n_heads,config.n_embds) for _ in range(config.n_decoder_layer)]), 
      ln=nn.LayerNorm(config.n_embds)
    ))

    '''Decoding the information loaded encoded token sequence via transformation.'''
    self.lm_head=nn.Linear(config.n_embds,config.vocab_size)

    t=self.transformer
    self.input=lambda xb,pos_xb:t.embd_drop((t.t_em(xb)+t.p_em(pos_xb)))
    self.output=lambda x:self.lm_head(t.ln(x))

    #applying generic _init_weight() function to each layer and sublayer of the model 
    self.apply(self._init_weight)

    #applying custome initialization to residual projection layer according to GPT-2 paper
    '''scaling the initialization by a factor i.e  1/sqrt(number of residual connection) '''
    for param_name,param_value in self.named_parameters():
      if param_name.endswith('proj.weight'):
        nn.init.normal_(param_value,mean=0.0,std=1.0/math.sqrt(2*n_decoder_layer))
    # printing total umber of parameters
    total_param=sum([param.numel() for param in self.parameters()])
    print(f'Number of parameter {total_param/1e6:.2f}M')

  '''helps to initialize the weights and biases of the layer by filling out values taken from 
  a scaled distribution ''' 
  def _init_weight(self,layer):
    if isinstance(layer,nn.Embedding):
      nn.init.normal_(layer.weight,mean=0.0,std=1.0)
    elif isinstance(layer,nn.Linear):
      nn.init.normal_(layer.weight,mean=0.0,std=1.0)
      if layer.bias is not None:
        nn.init.zeros_(layer.bias)
    elif isinstance(layer,nn.LayerNorm):
      nn.init.ones_(layer.weight)
      nn.init.zeros_(layer.bias)

  def configure_optimizer(self,train_config):
    
    decay_set=set()
    nondecay_set=set()
    decay_module=(nn.Linear,)
    nondecay_module=(nn.Embedding,nn.LayerNorm)
    for mn,m in self.named_modules():
      for pn,p in m.named_parameters():
        fpn= '{}.{}'.format(mn,pn) if mn else pn

        if fpn.endswith('weight') and isinstance(fpn,decay_module):
          decay_set.add(fpn)
        elif fpn.endswith('bias') or (fpn.endswith('weight') and isinstance(fpn,nondecay_module)):
          nondecay_set.add(fpn)
        
    #verify your seperation
    param={pn:p for pn,p in self.named_parameters()}
    union=decay_set | nondecay_set
    intersect=decay_set & nondecay_set
    assert len(intersect)==0,'some parameters {} are in both weight deacay set and non-weight decay set'.format(str(intersect))
    assert len(param.keys()-union)==0,'some parameters {} are out of scope of the existing model parameters'.format(str(param-union))  

    decay_set=sorted(list(decay_set))
    nondecay_set=sorted(list(nondecay_set))
    decay_param=[param[pn] for pn in decay_set]
    nondecay_param=[param[pn] for pn in nondecay_set]
    param_group=[
      {'params':decay_param,'weight_decay':train_config.weight_decay},
      {'params':nondecay_param,'weight_decay':0.0}
    ]
    
    optimizer=tt.optim.AdamW(param_group,lr=train_config.learning_rate,betas=train_config.betas)
    return optimizer
  
  def forward(self,xb,yb=None):
    B,T=xb.shape
    pos_xb=tt.arange(T,dtype=tt.long,device=device) # (T)

    '''token_embd=self.transformer.t_em(xb) #(B,T) -> (B,T,n_embds)
    position_embd=self.transformer.p_em(pos_xb) #(T) -> (T,n_embds)
    x=token_embd+position_embd # (B,T,n_embds) ----[Broadcasting rules]
    x=self.transformer.embd_drop(x)'''
    
    # (B,T,n_embds) + (T,n_embds) -> (B,T,n_embds) ----[Broadcasting rules]
    x=self.input(xb,pos_xb)
    
    # Transformer block in sequencial manner.
    for Transformer_block in self.transformer.block:
      x=Transformer_block(x)
    
    # (B,T,n_embds) -> (B,T,vocab_size)
    logits=self.output(x) 

    '''x=self.ln_layer(x)
    logits=self.lm_head(x)'''  

    if yb==None:
      loss=None
    else:
      # xb -> (B,T)
      # yb -> (B,T)
      # yb'(logits) -> (B,T,vocab_size)
      B,T,C=logits.shape
      yb=yb.view(B*T)
      logits=logits.view(B*T,C)

      #negative log likelihood loss / nll
      loss=F.cross_entropy(logits,yb)
    return logits,loss
  
  @tt.no_grad()
  def generate(self,idx,max_token):

    for _ in range(max_token):

      idx_crop=idx[:,-block_size:]

      logits,_=self(idx_crop) #(B,T) -> (B,T,vocab_size)

      logits=logits[:,-1,:] #(B,T,vocab_size) -> (B,vocab_size)

      prob=F.softmax(logits,dim=-1) #(B,vocab_size) -> (B,vocab_size)

      next_idx=tt.multinomial(prob,num_samples=1) #(B,vocab_size) -> (B,1)

      idx=tt.cat((idx,next_idx),dim=-1) #(B,T) ->(B,T+1)

    return idx

model=nanogptmodel()
model=model.to(device)
logits,loss=model(xb,yb)
print(logits.shape)
print(loss)

context=tt.zeros((1,1), dtype=tt.long,device=device)
print(decode(model.generate(context,max_token)[0].tolist()))

@tt.no_grad()
def evaluate_loss():
  model.eval()
  losses={}
  for data_type in ('train','val'):
    loss_array=tt.zeros((eval_iter,))
    for i in range(eval_iter):
      xb,yb=get_batch(data_type)
      logits,loss=model(xb,yb)
      loss_array[i]=loss.item()
    losses[data_type+'_loss']=loss_array.mean().item()
  model.train()
  return losses

optimizer=tt.optim.AdamW(model.parameters(),lr=learning_rate)

start_time=time.time()
for i in range(epochs):

  if i%eval_interval==0:
    loss=evaluate_loss()
    print(f"Step: {i}, train_loss: {loss['train_loss']:.4f}, val_loss: {loss['val_loss']:.4f}")

  #forward pass
  xb,yb=get_batch('train')
  logits,loss=model(xb,yb)

  #backpropagation
  optimizer.zero_grad(set_to_none=True)
  loss.backward()

  #update parameters
  optimizer.step()
end_time=time.time()

print(f'Training time: {end_time-start_time:.2f} sec')
print(decode(model.generate(context,max_token)[0].tolist()))

"""1. bigram model performance -> Train_loss: 2.4691, Val_loss: 2.4889.
2. single head self attention block performance -> train_loss: 2.1595, val_loss: 2.1689.
3. multi head self attention block performance -> train_loss: 0.6562, val_loss: 0.6810.
4. multi head self attention block +  Position-wise Feed-Forward Networks performance -> train_loss: 0.6378, val_loss: 0.6548.
5. 3 transformer block of point 4 -> train_loss: 0.3761, val_loss: 0.3830.
6. 3 transformer block of point 4 with skip connection in each block -> train_loss: 0.3154, val_loss: 0.3265, NOTE: point 5 losses were reached in approx epochs/2 training time.
7. Added projection layer for both multihead and FFNN and added 1 hidden layer in FFNN -> train_loss: 0.2998, val_loss: 0.3077.
8. Decoder only transformer : train_loss: 0.2257, val_loss: 0.2442
8. Encoder only transformer : train_loss: 0.2098, val_loss: 0.2337
"""

# with open('decoder_only_output.txt','w') as out_file:
#   out_file.write(decode(model.generate(context,max_token*2)[0].tolist()))