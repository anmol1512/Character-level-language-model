# -*- coding: utf-8 -*-
"""character-level Transformer LM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZ5sxia_jzwijlV5TXL6HGzjX2-hyZNo

## Decoder only Tranformer
"""

import torch as tt
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import time
import math
from ..utils.config import CfgNode as CN

batch_size= 128
block_size=256
learning_rate=1e-4
epochs=5000
eval_interval=500
eval_iter=300
n_embds=512
n_heads=8
max_token=2000
n_decoder_layer=8

attn_drop_ratio=0.2
proj_drop_ratio=0.2
embd_drop_ratio=0.2

class NewGELU(nn.Module):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
    """
    def forward(self, x):
        return 0.5 * x * (1.0 + tt.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))

"""**Multi-Head self attention**

In the context of the multi-head self-attention mechanism in the Transformer architecture, the idea is to project the input into multiple subspaces (heads), each with its own learnable parameters. Each head computes a separate attention distribution, capturing different aspects of the relationships within the input sequence. The outputs from all heads are then concatenated and linearly transformed to produce the final output.

**Advantages**
1.  It acts as a form of regularization, making the model less sensitive to noise in the training data.
2. The attention weights produced by different heads can provide insights into which parts of the input sequence are relevant for specific heads.
3. Parallelization or parallel processing is computationally efficient in terms of usage of hardware resources and training time.
4. Reduce overfitting and makes model more robust and capable of generalizing well to different types of data
"""


""" **Feed forward neural network**
1. A feedforward neural network is a type of artificial neural network in which nodes' connections do not form a loop.
2. Often referred to as a multi-layered network of neurons.
3. feedforward neural networks are so named because all information flows in a forward manner only.
"""

class SelfAttention(nn.Module):
  '''Communication mechanism so that tokens can attend to its other tokens.'''

  def __init__(self,config: CN) -> None:
    super().__init__()
    self.n_heads = config.n_heads
    self.n_embds = config.n_embds
    assert self.n_embds%self.n_heads == 0

    self.query = nn.Linear(self.n_embds, self.n_embds, bias=False)
    self.key = nn.Linear(self.n_embds, self.n_embds, bias=False)
    self.value = nn.Linear(self.n_embds, self.n_embds, bias=False)

    self.attn_dropout = nn.Dropout(config.attn_drop_ratio)
    self.resid_dropout = nn.Dropout(config.resid_drop_ratio)
    self.proj=nn.Linear(self.n_embds,self.n_embds)

    self.register_buffer('lower_tri',tt.tril(tt.ones(1,1,config.block_size,config.block_size,dtype=tt.long)))
    # lower triangular matrix is created and registered as buffer.

  def forward(self,q_input,k_input,v_input,pad = None,mask = None):
    #q_input , k_input, v_input dim's -> (batch size,sequence length,embedding dimention)
    B,T,C = v.shape 
    '''parallel processing of all the query for n_heads'''
    q = self.query(q_input) # parallel processing of all the query for n_heads
    k = self.key(k_input) # parallel processing of all the key for n_heads
    v = self.value(v_input) # parallel processing of all the value for n_heads

    n_heads = self.n_heads
    q = q.view(B,T,n_heads,self.n_embds//n_heads).transpose(1,2) #(B,n_heads,T,head_size)
    k = k.view(B,T,n_heads,self.n_embds//n_heads).transpose(1,2) #(B,n_heads,T,head_size)
    v = v.view(B,T,n_heads,self.n_embds//n_heads).transpose(1,2) #(B,n_heads,T,head_size)
    
    wei = q@k.transpose(-2,-1) # Dot product (compatibility matrix) (B,n_heads,T,T)
    wei = wei / math.sqrt(q.shape[-1]) # Scaled dot product (B,n_heads,T,T)
    
    if pad is not None and mask is not None:
      mask_layer = pad & self.lower_tri # (B,1,1,T) & (1,1,T,T) -> (B,1,T,T)
      wei = wei.masked_fill(mask_layer[:,:,:T,:T] == 0, float('-inf'))
    elif pad is not None:
        wei = wei.masked_fill(pad[:,:,:,:T] == 0, float('-inf'))
    elif mask is not None:
        wei = wei.masked_fill(self.lower_tri[:,:,:T,:T] == 0, float('-inf'))
    
    wei = F.softmax(wei,dim=-1) # (B,n_heads,T,T)
    wei = self.attn_dropout(wei) # (B,n_heads,T,T)
    
    out = wei @ v # # (B,n_heads,T,T) @ # (B,n_heads,T,head_size) -> (B,n_heads,T,head_size)
    out = out.transpose(1,2).contiguous().view(B,T,C) # (B,T,n_embds)
    out=self.resid_dropout(self.proj(out)) # (B,T,n_embds)
    return out

#([uni or bi] multi-head SA +FFNN)
# communication followed by computation
class Encoder(nn.Module):
  '''
  Encoder Block with no masking in self attention block

  Followed by,feed forward neural network 

  Note: Pre-activation layer normalization for residual connection for SA and FFNN 
  '''
  def __init__(self, config: CN) -> None:
    super().__init__()
    
    self.sa_heads = SelfAttention(config)
    # Feed forward neural network block to think on the comunicated information
    # operating on token level individually
    n_embds = config.n_embds
    self.ann = nn.ModuleDict(dict(
      hidden = nn.Linear(n_embds,4*n_embds),
      proj = nn.Linear(4*n_embds,n_embds),
      act = nn.ReLU(),
      neural_dropout = nn.Dropout(config.resid_drop_ratio)
    ))

    self.ln1 = nn.LayerNorm(n_embds)
    self.ln2 = nn.LayerNorm(n_embds)

    ffnn = self.ann
    self.neural_net = lambda x : ffnn.neural_dropout(ffnn.proj(ffnn.act(ffnn.hidden(x))))

  def forward(self,x,pad = None):
    norm_x = self.ln1(x)
    x = x + self.sa_heads(norm_x,norm_x,norm_x,pad)

    norm_x = self.ln2(x)
    x = x + self.neural_net(norm_x)
    return x

class Decoder(nn.Module):
  '''
  Note: output embedding is taken as input to decode block
  which is unconditioned and is also reffered as teacher - forcing when we deal with RNN Decoder architecture

  Decoder Block with masking in self attention block

  Followed by, cross attention [i.e query from output of first attention and key,value from output of encoder]
  
  Followed by, feed forward neural network 

  Note: Pre-activation layer normalization for residual connection for SA and FFNN 
  '''
  def __init__(self, config: CN) -> None:
    super().__init__()
    
    self.sa_heads = SelfAttention(config)
    self.ca_heads = SelfAttention(config)
    # Feed forward neural network block to think on the comunicated information
    # operating on token level individually
    n_embds = config.n_embds
    self.ann = nn.ModuleDict(dict(
      hidden = nn.Linear(n_embds,4*n_embds),
      proj = nn.Linear(4*n_embds,n_embds),
      act = nn.ReLU(),
      neural_dropout = nn.Dropout(config.resid_drop_ratio)
    ))

    self.ln1 = nn.LayerNorm(n_embds)
    self.ln2 = nn.LayerNorm(n_embds)
    self.ln3 = nn.LayerNorm(n_embds)
    self.ln4 = nn.LayerNorm(n_embds)

    ffnn = self.ann
    self.neural_net = lambda x : ffnn.neural_dropout(ffnn.proj(ffnn.act(ffnn.hidden(x))))

  def forward(self,x,encoder_x,pad = None,mask = True):
    norm_x = self.ln1(x)
    x = x + self.sa_heads(norm_x,norm_x,norm_x,pad,mask)
    
    norm_x = self.ln2(x)
    norm_encoder_x = self.ln3(encoder_x)
    x = x + self.ca_heads(q_input = norm_x, k_input = norm_encoder_x,v_input = norm_encoder_x)
    ''' 
    In the case of the cross-attention layer in the decoder, where queries come from the previous decoder layer
    and keys and values come from the output of the encoder, masking the future tokens is not required.
    This is because, in the decoder's cross-attention, you want the model to attend to all positions in the input sequence.
    '''
    norm_x = self.ln4(x)
    x = x + self.neural_net(norm_x)
    return x

class NanoGPTModel(nn.Module):
  '''Transformer Language Model'''

  @staticmethod
  def get_default_config():
    '''All Hyperparameters related to transformer model'''
    config=CN()
    # either model_type or (n_decoder_layer,n_heads,n_embds) should be given
    config.n_block=None
    config.n_heads=None
    config.n_embds=None

    #below config should be provided externally
    config.vocab_size=None
    config.block_size=None
    
    #Dropout Hyperparameters
    config.attn_drop_ratio=0.2
    config.resid_drop_ratio=0.5
    config.embd_drop_ratio=0.2

    config.device = 'auto'
    return config

  def __init__(self,config):
    super().__init__()

    self.config=config
    assert config.vocab_size is not None
    assert config.block_size is not None

    block_params_checker = all([config.n_decoder_layer is not None,config.n_heads is not None,config.n_embds is not None])
    assert block_params_checker

    if config.device == 'auto':
      self.device=tt.device('cuda:0') if tt.cuda.is_available() else tt.device('cpu')
    else:
       self.device=config.device

    '''Encoding the input token sequence t_em with position encoding p_em'''
    '''then applying 'n_decoder_layer' transformer block followed by a layer normalization layer'''
    self.t_em = nn.Embedding(config.vocab_size,config.n_embds,padding_idx = 0) 
    self.p_em = nn.Embedding(config.block_size,config.n_embds)
    self.embd_dropout = nn.Dropout(config.embd_drop_ratio)
    
    self.encoder_blocks = nn.ModuleList([Encoder(config) for _ in range(config.n_block)]) 
    self.decoder_blocks = nn.ModuleList([Decoder(config) for _ in range(config.n_block)])
    
    self.ln = nn.LayerNorm(config.n_embds)
    '''Decoding the information loaded encoded token sequence via transformation.'''
    self.linear_layer = nn.Linear(config.n_embds,config.vocab_size,bias = False)

    '''Weight Initialization to avoid exploding/vanishing gradient problem'''
    #applying generic _init_weight() function to each layer and sublayer of the model 
    self.apply(self._init_weight)

    #applying custome initialization to residual projection layer according to GPT-2 paper
    '''scaling the initialization by a factor i.e  1/sqrt(number of residual connection) '''
    for param_name,param_value in self.named_parameters():
      if param_name.endswith('proj.weight'):
        nn.init.normal_(param_value,mean=0.0,std=0.02/math.sqrt(5*config.n_block))
    
    self.linear_layer.weight.data = self.t_em.weight.data.t()
    
    # printing total number of parameters
    self.total_param=sum([param.numel() for param in self.parameters()])
    print(f'Number of model parameters: {self.total_param/1e6:.2f}M')

  def get_num_parameters(self):
    return self.total_param
  
  def _init_weight(self,layer):
    '''
    Helps to initialize the weights and biases of the layer 
    
    Filling out values taken from a scaled distribution 
    '''
    if isinstance(layer,nn.Embedding):
      nn.init.normal_(layer.weight,mean=0.0,std=0.02/math.sqrt(self.config.n_embds))
    elif isinstance(layer,nn.Linear):
      nn.init.normal_(layer.weight,mean=0.0,std=0.02)
      if layer.bias is not None:
        nn.init.zeros_(layer.bias)
    elif isinstance(layer,nn.LayerNorm):
      nn.init.ones_(layer.weight)
      nn.init.zeros_(layer.bias)

  def configure_optimizer(self,train_config):
    decay_set=set()
    nondecay_set=set()
    decay_module=(nn.Linear,)
    nondecay_module=(nn.Embedding,nn.LayerNorm)
    for mn,m in self.named_modules():
      for pn,p in m.named_parameters():
        fpn= '{}.{}'.format(mn,pn) if mn else pn

        if fpn.endswith('weight') and isinstance(fpn,decay_module):
          decay_set.add(fpn)
        elif fpn.endswith('bias') or (fpn.endswith('weight') and isinstance(fpn,nondecay_module)):
          nondecay_set.add(fpn)
        
    #verify your seperation
    param={pn:p for pn,p in self.named_parameters()}
    union=decay_set | nondecay_set
    intersect=decay_set & nondecay_set
    assert len(intersect)==0,'some parameters {} are in both weight deacay set and non-weight decay set'.format(str(intersect))
    assert len(param.keys()-union)==0,'some parameters {} are out of scope of the existing model parameters'.format(str(param-union))  

    decay_set=sorted(list(decay_set))
    nondecay_set=sorted(list(nondecay_set))
    decay_param=[param[pn] for pn in decay_set]
    nondecay_param=[param[pn] for pn in nondecay_set]
    param_group=[
      {'params':decay_param,'weight_decay':train_config.weight_decay},
      {'params':nondecay_param,'weight_decay':0.0}
    ]
    
    optimizer=tt.optim.AdamW(param_group,lr=train_config.learning_rate,betas=train_config.betas)
    return optimizer
  
  def encode(self,idx):
    B,T = idx.shape #idx is the language we need to translate
    pad = (idx != 0).unsqueeze(1).unsqueeze(2) # (B,1,1,T)
    pos_idx=tt.arange(T,dtype=tt.long,device = self.device).unsqueeze(0) # (1,T)

    # Input Embedding
    token_embd = self.t_em(idx) #convert the input tokens to vectors of dimension n_embds
    position_embd = self.p_em(pos_idx)
    x = self.embd_dropout(token_embd + position_embd) # (B,T,n_embds) + (1,T,n_embds) -> (B,T,n_embds) ----[Broadcasting rules]
    
    for block in self.encoder_blocks:
      x = block(x,pad = pad)

    return x
  
  def decode(self,idx, encoded_x):
    B,T = idx.shape #idx is the target translated language
    pad = (idx != 0).unsqueeze(1).unsqueeze(2) # (B,1,1,T)
    pos_idx=tt.arange(T,dtype=tt.long,device = self.device).unsqueeze(0) # (1,T)

    '''both input embedding and ouput embedding 
    share the same 
    token embedding weights and position embedding weights'''
    # Output Embedding
    token_embd = self.t_em(idx) #convert the output tokens to vectors of dimension n_embds
    position_embd = self.p_em(pos_idx)
    y = self.embd_dropout(token_embd + position_embd) # (B,T,n_embds) + (1,T,n_embds) -> (B,T,n_embds) ----[Broadcasting rules]

    for block in self.decoder_blocks:
      y = block(y,encoded_x,pad)

    '''
    pre-activation layer normalization, 
    followed by linear transformation
    '''
    y = self.ln(y)
    logits = self.linear_layer(y) # (B,T,n_embds) -> (B,T,vocab_size)
    return logits
  
  def forward(self,xb,yb=None):

    encoded_x = self.encode(xb)
    logits = self.decode(yb, encoded_x)
    # xb -> (B,T)
    # yb -> (B,T)
    # yb'(logits) -> (B,T,vocab_size)
    B,T,C=logits.shape
    yb=yb.view(B*T)
    logits=logits.view(B*T,C)

    #negative log likelihood loss / nll
    loss=F.cross_entropy(logits,yb)
    return logits,loss
  
  @tt.no_grad()
  def generate(self,idx,max_token):

    for _ in range(max_token):

      idx_crop=idx[:,-block_size:]

      logits,_=self(idx_crop) #(B,T) -> (B,T,vocab_size)

      logits=logits[:,-1,:] #(B,T,vocab_size) -> (B,vocab_size)

      prob=F.softmax(logits,dim=-1) #(B,vocab_size) -> (B,vocab_size)

      next_idx=tt.multinomial(prob,num_samples=1) #(B,vocab_size) -> (B,1)

      idx=tt.cat((idx,next_idx),dim=-1) #(B,T) ->(B,T+1)

    return idx

# model=nanogptmodel()
# model=model.to(device)
# logits,loss=model(xb,yb)
# print(logits.shape)
# print(loss)

# context=tt.zeros((1,1), dtype=tt.long,device=device)
# print(decode(model.generate(context,max_token)[0].tolist()))

# @tt.no_grad()
# def evaluate_loss():
#   model.eval()
#   losses={}
#   for data_type in ('train','val'):
#     loss_array=tt.zeros((eval_iter,))
#     for i in range(eval_iter):
#       xb,yb=get_batch(data_type)
#       logits,loss=model(xb,yb)
#       loss_array[i]=loss.item()
#     losses[data_type+'_loss']=loss_array.mean().item()
#   model.train()
#   return losses

# optimizer=tt.optim.AdamW(model.parameters(),lr=learning_rate)

# start_time=time.time()
# for i in range(epochs):

#   if i%eval_interval==0:
#     loss=evaluate_loss()
#     print(f"Step: {i}, train_loss: {loss['train_loss']:.4f}, val_loss: {loss['val_loss']:.4f}")

#   #forward pass
#   xb,yb=get_batch('train')
#   logits,loss=model(xb,yb)

#   #backpropagation
#   optimizer.zero_grad(set_to_none=True)
#   loss.backward()

#   #update parameters
#   optimizer.step()
# end_time=time.time()

# print(f'Training time: {end_time-start_time:.2f} sec')
# print(decode(model.generate(context,max_token)[0].tolist()))

"""1. bigram model performance -> Train_loss: 2.4691, Val_loss: 2.4889.
2. single head self attention block performance -> train_loss: 2.1595, val_loss: 2.1689.
3. multi head self attention block performance -> train_loss: 0.6562, val_loss: 0.6810.
4. multi head self attention block +  Position-wise Feed-Forward Networks performance -> train_loss: 0.6378, val_loss: 0.6548.
5. 3 transformer block of point 4 -> train_loss: 0.3761, val_loss: 0.3830.
6. 3 transformer block of point 4 with skip connection in each block -> train_loss: 0.3154, val_loss: 0.3265, NOTE: point 5 losses were reached in approx epochs/2 training time.
7. Added projection layer for both multihead and FFNN and added 1 hidden layer in FFNN -> train_loss: 0.2998, val_loss: 0.3077.
8. Decoder only transformer : train_loss: 0.2257, val_loss: 0.2442
8. Encoder only transformer : train_loss: 0.2098, val_loss: 0.2337
"""

# with open('decoder_only_output.txt','w') as out_file:
#   out_file.write(decode(model.generate(context,max_token*2)[0].tolist()))